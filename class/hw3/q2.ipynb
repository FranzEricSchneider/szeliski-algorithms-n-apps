{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcaa9b92e2fb0f2186de6aafa729b5f1",
     "grade": false,
     "grade_id": "cell-6c9f551ea02c7bf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Neural Networks for Recognition - Assignment 3\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs: Qichen(Lead), Paritosh, Rawal, Yan, Zen, Wen-Hsuan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2ec726294de647658dbb456ef6bcc4f",
     "grade": false,
     "grade_id": "cell-372b7e8bfa64e2d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2 Implement a Fully Connected Network (65 points + 10 Extra Credit)\n",
    "\n",
    "**Please include all the write up answers below to theory.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10a52a255fcde49ea866eed7aab3f8fd",
     "grade": false,
     "grade_id": "cell-2d56327246e23156",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do Not Modify\n",
    "# Do Not Import ANY other packages\n",
    "import numpy as np\n",
    "\n",
    "# use for a \"no activation\" layer\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_deriv(post_act):\n",
    "    return np.ones_like(post_act)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(post_act):\n",
    "    return 1-post_act**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bca389a300d5c03ed615eb39d600428b",
     "grade": false,
     "grade_id": "cell-cf0ebdeb56cae121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.1 Network Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0d8eae8a316873621dd3d282fd25b9c",
     "grade": false,
     "grade_id": "cell-810587ffe33a3598",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.1 (3 points WriteUp)\n",
    "Why is it not a good idea to initialize a network with all zeros? If you imagine that every layer has weights and biases, what can a zero-initialized network output after training?\n",
    "\n",
    "<font color=\"red\">**Please include the write up answer to theory.ipynb**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b93bec685cef7b45bd7855cd6c19856",
     "grade": false,
     "grade_id": "cell-72609a649db69760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.2 (3 points Autograder)\n",
    "Implement `initialize_weights` below to initialize neural network weights with [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), where $Var[w] = \\frac{2}{n_{in}+ n_{out}} $ where $n$ is the dimensionality of the vectors and you use a **uniform distribution** to sample random numbers (see eq 16 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9f344fcf8cb029b398a571f60397a55",
     "grade": false,
     "grade_id": "cell-04dbce39e616d009",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(in_size: int, out_size: int, params: dict, name: str='' ):\n",
    "    '''\n",
    "    Initialize the weights W and b for a linear layer Y = XW + b\n",
    "    \n",
    "    [input]\n",
    "    * in_size -- the feature dimension of the input\n",
    "    * out_size -- the feature dimension of the output\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    \n",
    "    HINTS:\n",
    "    (1) b should be a 1D array, not a 2D array with a singleton dimension\n",
    "    '''\n",
    "    W, b = None, None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    xavier_radius = np.sqrt(6) / np.sqrt(in_size + out_size)\n",
    "    W = np.random.uniform(low=-xavier_radius, high=xavier_radius, size=(in_size, out_size))\n",
    "\n",
    "    # It's not super clear, but I think in the paper biases are initialized as 0\n",
    "    b = np.zeros(out_size)\n",
    "\n",
    "    params['W' + name] = W\n",
    "    params['b' + name] = b\n",
    "\n",
    "\n",
    "# params = {}\n",
    "# initialize_weights(in_size=2, out_size=3, params=params, name=\"foo\")\n",
    "# initialize_weights(in_size=4, out_size=2, params=params, name=\"bar\")\n",
    "# for key, value in params.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaee36111de32c3bfa095826ba2ad0d0",
     "grade": true,
     "grade_id": "q2_1_2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "initialize_weights(2,25,params,'layer1')\n",
    "initialize_weights(25,4,params,'output')\n",
    "assert(params['Wlayer1'].shape == (2,25))\n",
    "assert(params['blayer1'].shape == (25,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5806373b1f7bdaee8f96dd954e32d5e",
     "grade": false,
     "grade_id": "cell-50fa916b36662a34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.1.3 (2 points WriteUp)\n",
    "Why do we initialize with random numbers? Why do we scale the initialization depending on layer size (see near Fig 6 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))?\n",
    "\n",
    "<font color=\"red\">**Please include the write up answer to theory.ipynb**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0b7d79263e7201b557a59b18b451fc3",
     "grade": false,
     "grade_id": "cell-e1c75223ce84f014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.2 Forward Propagation\n",
    "\n",
    "The appendix (in `theory.jpynb`) has the math for forward propagation, we will implement it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e060271ef9b2126cc0f0d4946a294fb9",
     "grade": false,
     "grade_id": "cell-edc97acfe64d2a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.2.1 (12 points Autograder)\n",
    "Implement `sigmoid`, along with `forward` propagation for a single layer with an activation function, namely\n",
    "$y = \\sigma(X W + b)$, returning the output and intermediate results for an $N \\times D$ dimension input $X$, with examples along the rows, data dimensions along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ee1ee0d27dbbf90dee503697e7364a4",
     "grade": false,
     "grade_id": "cell-9d629c29b34231c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(X: np.ndarray):\n",
    "    '''\n",
    "    A sigmoid activation function\n",
    "    \n",
    "    [input]\n",
    "    * X -- input data [N x D]\n",
    "    \n",
    "    [output]\n",
    "    * res -- output after the sigmoid function\n",
    "    '''\n",
    "    res = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    res = 1 / (1 + np.exp(-X))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# a = np.array([[1, 2, -3], [-4, -5, 6]])\n",
    "# print(sigmoid(a))\n",
    "# print(np.exp(4))\n",
    "# print(1 / (1 + np.exp(4)))\n",
    "\n",
    "# from matplotlib import pyplot\n",
    "# plot_x = np.linspace(-10, 10, 1000)\n",
    "# plot_y = sigmoid(plot_x)\n",
    "# pyplot.plot(plot_x, plot_y)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6946729b698b44b8a45c8c0a2282809",
     "grade": true,
     "grade_id": "q2_2_1_sigmoid",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test = sigmoid(np.array([-100,100]))\n",
    "assert test.min() < 1e-3\n",
    "assert test.max() > 1 - 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d5e5b794440965513350df9446715a8",
     "grade": false,
     "grade_id": "cell-8a7b5e6897a9d712",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward(X: np.ndarray, params: dict, name: str='',\n",
    "            activation: callable=sigmoid):\n",
    "    \"\"\"\n",
    "    Do a forward pass\n",
    "\n",
    "    [input]\n",
    "    * X -- input data [N x D]\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    * activation -- the activation function (default is sigmoid)\n",
    "    \n",
    "    [output]\n",
    "    * post_act -- output after a linear layer and activation\n",
    "    \"\"\"\n",
    "    pre_act, post_act = None, None\n",
    "    # get the layer parameters\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    pre_act = X @ W + b\n",
    "    post_act = activation(pre_act)\n",
    "\n",
    "    # store the pre-activation and post-activation values\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, pre_act, post_act)\n",
    "\n",
    "    return post_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc7c085bce021af019d0c5e92a579487",
     "grade": true,
     "grade_id": "q2_2_1_forward",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = {'Wlayer1': np.random.rand(10, 25), 'blayer1': np.random.rand(25,)}\n",
    "X = np.random.rand(3, 10)\n",
    "y = forward(X, params, 'layer1')\n",
    "assert 'cache_layer1' in params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f7929b5fb065fc93bc1c42b533bb010",
     "grade": false,
     "grade_id": "cell-e82fe9a6f4b520d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.2.2 (5 points Autograder)\n",
    "Implement the `softmax` function. Be sure the use the numerical stability trick you derived in Q1.1 softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5020c50b9b2c866cb6575c56e5f4af0",
     "grade": false,
     "grade_id": "cell-e41cfc344ff8bdef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    A softmax function.\n",
    "    \n",
    "    [input] \n",
    "    * X -- input data [N x D]\n",
    "    \n",
    "    [output]\n",
    "    * res -- values after softmax\n",
    "    \"\"\"\n",
    "    res = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    vector_shape = (X.shape[0], 1)\n",
    "    max_x = np.max(X, axis=1).reshape(vector_shape)\n",
    "    values = np.exp(X - max_x)\n",
    "    summed = np.sum(values, axis=1).reshape(vector_shape)\n",
    "    res = values / summed\n",
    "    \n",
    "    return res\n",
    "\n",
    "# x = np.array([[-100, -10, -1], [0, 1, 2]])\n",
    "# print(softmax(x))\n",
    "# print(softmax(10 + x))\n",
    "# print(softmax(1000 + x))\n",
    "# print(np.sum(softmax(x), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "514c4d57a2c4f5e6e9d26fb33e9ad740",
     "grade": true,
     "grade_id": "q2_2_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cec15c2781070d29e254f157979d9c25",
     "grade": false,
     "grade_id": "cell-ca7affc876fccf2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.2.3 (5 points Autograder)\n",
    "Implement `compute_loss_and_acc` to compute the accuracy of a set of labels, along with the scalar loss across the data.  The loss function generally used for classification is the cross-entropy loss.\n",
    "\n",
    "$$L_{\\textbf{f}}(\\textbf{D}) = - \\sum_{(\\textbf{x}, \\textbf{y})\\in \\textbf{D}}\\textbf{y}\\cdot\\log(\\textbf{f}(\\textbf{x}))$$\n",
    "Here $\\textbf{D}$ is the full training dataset of data samples $\\textbf{x}$ ($N\\times 1$ vectors, N = dimensionality of data) and labels $\\textbf{y}$ ($C\\times 1$ one-hot vectors, C = number of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "427d2e6e94421f78f33dd01e1d974fa4",
     "grade": false,
     "grade_id": "cell-e8d3b0354b03c98c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss_and_acc(y: np.ndarray, probs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute total loss and accuracy\n",
    "    \n",
    "    [input]\n",
    "    * y -- one hot labels [N x C]\n",
    "    * probs -- class probabities [N x C]\n",
    "    \n",
    "    [output]\n",
    "    * loss -- cross-entropy loss\n",
    "    * acc -- accuracy\n",
    "    \"\"\"\n",
    "    loss, acc = None, None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    loss = -np.sum(y * np.log(probs))\n",
    "\n",
    "    actual_classes = np.where(y)[1]\n",
    "    chosen_classes = np.argmax(probs, axis=1)\n",
    "    acc = np.sum(actual_classes == chosen_classes) / y.shape[0]\n",
    "    \n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "# y = np.array([\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 0, 0, 1],\n",
    "#     [0, 0, 1, 0],\n",
    "#     [0, 0, 0, 1],\n",
    "#     [0, 1, 0, 0],\n",
    "# ])\n",
    "# probs = np.array([\n",
    "#     [0.9, 0.05, 0.03, 0.02],\n",
    "#     [0.2, 0.1, 0.1, 0.6],\n",
    "#     [0.97, 0.01, 0.01, 0.01],\n",
    "#     [0.05, 0.05, 0.1, 0.8],\n",
    "#     [0.05, 0.4, 0.5, 0.05],\n",
    "# ])\n",
    "# print(compute_loss_and_acc(y, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d12a9791e1272b124d81facc47b2416",
     "grade": true,
     "grade_id": "q2_2_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7059ecdfb9cdaddb55cf94f82e91e64b",
     "grade": false,
     "grade_id": "cell-32109cdbdb894d52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.3 Backwards Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89831bd8123b2288d1934e97874837f5",
     "grade": false,
     "grade_id": "cell-f50e629f2d59ad01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.3.1 (10 points Autograder)\n",
    "Compute back-propagation for a single layer, given the original weights, the appropriate intermediate results, and given gradient with respect to the loss. You should return the gradient with respect to $X$ so you can feed it into the next layer. As a size check, your gradients should be the same dimensions as the original objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495f65a53090819a1c4ac3176b95c89c",
     "grade": false,
     "grade_id": "cell-05a2aee350a000f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_deriv(post_act: np.ndarray):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid.\n",
    "    \n",
    "    we give this to you because you proved it\n",
    "    it's a function of post_act\n",
    "    \"\"\"\n",
    "    res = post_act*(1.0-post_act)\n",
    "    return res\n",
    "\n",
    "\n",
    "def backwards(delta: np.ndarray, params: dict, name: str='',\n",
    "              activation_deriv: callable=sigmoid_deriv):\n",
    "    \"\"\"\n",
    "    Do a backwards pass\n",
    "\n",
    "    [input]\n",
    "    * delta -- errors to backpropparams\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    * activation_deriv -- the derivative of the activation_func\n",
    "    \n",
    "    [output]\n",
    "    * grad_X -- gradient w.r.t X\n",
    "    \"\"\"\n",
    "    grad_X, grad_W, grad_b = None, None, None\n",
    "    # everything you may need for this layer\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    X, pre_act, post_act = params['cache_' + name]\n",
    "\n",
    "    import ipdb; ipdb.set_trace()\n",
    "    \n",
    "    # do the derivative through activation first\n",
    "    # then compute the derivative W,b, and X\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # TODO\n",
    "    grad_b = delta\n",
    "\n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d07599281e04ed15aeb97dd1a468daf0",
     "grade": true,
     "grade_id": "q2_3_1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-13-c5b785d7ae89>\u001b[0m(37)\u001b[0;36mbackwards\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     36 \u001b[0;31m    \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 37 \u001b[0;31m    \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> W.shape\n",
      "(40, 20)\n",
      "ipdb> error.shape\n",
      "*** NameError: name 'error' is not defined\n",
      "ipdb> delta.shape\n",
      "(5, 20)\n",
      "ipdb> params.keys()\n",
      "dict_keys(['Wlayer1', 'blayer1', 'cache_layer1'])\n",
      "ipdb> W@X + b\n",
      "*** ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 20)\n",
      "ipdb> W.shape\n",
      "(40, 20)\n",
      "ipdb> x.shape\n",
      "*** NameError: name 'x' is not defined\n",
      "ipdb> X.shape\n",
      "(5, 40)\n",
      "ipdb> X @ W\n",
      "array([[12.38788829, 12.25083686, 11.54872991, 12.52141181, 11.05774277,\n",
      "        10.91819091, 12.95965162, 13.58089727,  9.70968604, 11.81682544,\n",
      "         8.79543332, 12.71606345, 11.07038616, 10.81220433, 10.6884641 ,\n",
      "        11.41017805, 11.40476189, 10.74396982, 10.43797814, 13.01508302],\n",
      "       [ 8.95020303,  9.76673083,  8.32107711,  9.17400328,  8.53997138,\n",
      "         8.20786817,  9.99753339, 10.44569724,  6.40460398,  8.27751425,\n",
      "         6.57597368,  8.90284223,  8.79991618,  7.89005589,  8.97231408,\n",
      "         8.91292651,  9.32919209,  7.97907708,  9.21682965,  9.51312647],\n",
      "       [10.01871783, 12.76856515, 11.56888228, 12.07679417, 10.91139874,\n",
      "         8.93823862, 12.67012103, 12.05349456,  9.77428721, 10.41329791,\n",
      "         9.29880785, 10.46379674,  9.71584701, 10.95111023, 11.40633033,\n",
      "        11.03708763, 10.80468127,  9.99968043, 10.51950159, 12.05432526],\n",
      "       [10.38414648, 11.79393172, 10.18160973, 10.77341733, 10.42490267,\n",
      "         9.95979887, 11.82717516, 12.05030775, 10.08053008,  9.32324146,\n",
      "         8.73315334, 11.01317749,  9.7938973 , 10.38824506, 10.62278852,\n",
      "        10.26960872, 10.4651912 ,  9.34411546, 10.51859056, 11.67377144],\n",
      "       [ 9.91389602, 11.88767166, 10.6380003 , 11.55440303, 10.46046816,\n",
      "         9.24543184, 11.89714198, 12.05819478,  8.62056329,  9.62266332,\n",
      "         8.28198189, 10.65894645,  8.8925788 , 10.2381001 ,  9.54757901,\n",
      "         9.7451377 , 10.25807784,  8.7894503 ,  8.93953284, 11.38718355]])\n",
      "ipdb> X @ W + b\n",
      "array([[12.68006395, 12.27430531, 12.34028102, 13.21879798, 11.31560563,\n",
      "        11.13991783, 13.27581741, 13.7264575 , 10.53444841, 11.86218189,\n",
      "         9.20615033, 13.11076308, 11.27923955, 11.46513646, 11.13834821,\n",
      "        11.74750284, 11.60164408, 10.83866052, 10.76621716, 13.20961077],\n",
      "       [ 9.24237869,  9.79019929,  9.11262822,  9.87138945,  8.79783425,\n",
      "         8.42959509, 10.31369918, 10.59125747,  7.22936635,  8.3228707 ,\n",
      "         6.9866907 ,  9.29754186,  9.00876956,  8.54298802,  9.42219818,\n",
      "         9.2502513 ,  9.52607428,  8.07376778,  9.54506867,  9.70765421],\n",
      "       [10.31089349, 12.79203361, 12.36043339, 12.77418033, 11.1692616 ,\n",
      "         9.15996554, 12.98628681, 12.19905479, 10.59904958, 10.45865435,\n",
      "         9.70952486, 10.85849637,  9.9247004 , 11.60404236, 11.85621444,\n",
      "        11.37441243, 11.00156346, 10.09437113, 10.8477406 , 12.24885301],\n",
      "       [10.67632214, 11.81740017, 10.97316084, 11.4708035 , 10.68276553,\n",
      "        10.18152579, 12.14334094, 12.19586798, 10.90529245,  9.3685979 ,\n",
      "         9.14387035, 11.40787712, 10.00275069, 11.04117719, 11.07267263,\n",
      "        10.60693351, 10.66207339,  9.43880616, 10.84682957, 11.86829919],\n",
      "       [10.20607168, 11.91114011, 11.42955141, 12.2517892 , 10.71833102,\n",
      "         9.46715876, 12.21330776, 12.20375501,  9.44532565,  9.66801976,\n",
      "         8.69269891, 11.05364608,  9.10143219, 10.89103224,  9.99746311,\n",
      "        10.08246249, 10.45496003,  8.884141  ,  9.26777186, 11.5817113 ]])\n",
      "ipdb> pre_act.shape\n",
      "(5, 20)\n",
      "ipdb> X @ W + b.shape\n",
      "array([[32.38788829, 32.25083686, 31.54872991, 32.52141181, 31.05774277,\n",
      "        30.91819091, 32.95965162, 33.58089727, 29.70968604, 31.81682544,\n",
      "        28.79543332, 32.71606345, 31.07038616, 30.81220433, 30.6884641 ,\n",
      "        31.41017805, 31.40476189, 30.74396982, 30.43797814, 33.01508302],\n",
      "       [28.95020303, 29.76673083, 28.32107711, 29.17400328, 28.53997138,\n",
      "        28.20786817, 29.99753339, 30.44569724, 26.40460398, 28.27751425,\n",
      "        26.57597368, 28.90284223, 28.79991618, 27.89005589, 28.97231408,\n",
      "        28.91292651, 29.32919209, 27.97907708, 29.21682965, 29.51312647],\n",
      "       [30.01871783, 32.76856515, 31.56888228, 32.07679417, 30.91139874,\n",
      "        28.93823862, 32.67012103, 32.05349456, 29.77428721, 30.41329791,\n",
      "        29.29880785, 30.46379674, 29.71584701, 30.95111023, 31.40633033,\n",
      "        31.03708763, 30.80468127, 29.99968043, 30.51950159, 32.05432526],\n",
      "       [30.38414648, 31.79393172, 30.18160973, 30.77341733, 30.42490267,\n",
      "        29.95979887, 31.82717516, 32.05030775, 30.08053008, 29.32324146,\n",
      "        28.73315334, 31.01317749, 29.7938973 , 30.38824506, 30.62278852,\n",
      "        30.26960872, 30.4651912 , 29.34411546, 30.51859056, 31.67377144],\n",
      "       [29.91389602, 31.88767166, 30.6380003 , 31.55440303, 30.46046816,\n",
      "        29.24543184, 31.89714198, 32.05819478, 28.62056329, 29.62266332,\n",
      "        28.28198189, 30.65894645, 28.8925788 , 30.2381001 , 29.54757901,\n",
      "        29.7451377 , 30.25807784, 28.7894503 , 28.93953284, 31.38718355]])\n",
      "ipdb> .shape(X @ W + b)\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> (X @ W + b).shape\n",
      "(5, 20)\n",
      "ipdb> np.allclose(pre_act, X @ W + b)\n",
      "False\n",
      "ipdb> X @ W + b\n",
      "array([[12.68006395, 12.27430531, 12.34028102, 13.21879798, 11.31560563,\n",
      "        11.13991783, 13.27581741, 13.7264575 , 10.53444841, 11.86218189,\n",
      "         9.20615033, 13.11076308, 11.27923955, 11.46513646, 11.13834821,\n",
      "        11.74750284, 11.60164408, 10.83866052, 10.76621716, 13.20961077],\n",
      "       [ 9.24237869,  9.79019929,  9.11262822,  9.87138945,  8.79783425,\n",
      "         8.42959509, 10.31369918, 10.59125747,  7.22936635,  8.3228707 ,\n",
      "         6.9866907 ,  9.29754186,  9.00876956,  8.54298802,  9.42219818,\n",
      "         9.2502513 ,  9.52607428,  8.07376778,  9.54506867,  9.70765421],\n",
      "       [10.31089349, 12.79203361, 12.36043339, 12.77418033, 11.1692616 ,\n",
      "         9.15996554, 12.98628681, 12.19905479, 10.59904958, 10.45865435,\n",
      "         9.70952486, 10.85849637,  9.9247004 , 11.60404236, 11.85621444,\n",
      "        11.37441243, 11.00156346, 10.09437113, 10.8477406 , 12.24885301],\n",
      "       [10.67632214, 11.81740017, 10.97316084, 11.4708035 , 10.68276553,\n",
      "        10.18152579, 12.14334094, 12.19586798, 10.90529245,  9.3685979 ,\n",
      "         9.14387035, 11.40787712, 10.00275069, 11.04117719, 11.07267263,\n",
      "        10.60693351, 10.66207339,  9.43880616, 10.84682957, 11.86829919],\n",
      "       [10.20607168, 11.91114011, 11.42955141, 12.2517892 , 10.71833102,\n",
      "         9.46715876, 12.21330776, 12.20375501,  9.44532565,  9.66801976,\n",
      "         8.69269891, 11.05364608,  9.10143219, 10.89103224,  9.99746311,\n",
      "        10.08246249, 10.45496003,  8.884141  ,  9.26777186, 11.5817113 ]])\n",
      "ipdb> pre_act\n",
      "array([[4.01574030e-01, 5.69960955e-01, 8.24325592e-01, 9.94283296e-01,\n",
      "        3.95246047e-01, 1.92882363e-01, 6.69611237e-01, 8.95930721e-01,\n",
      "        5.56273407e-02, 8.38450133e-01, 9.10262258e-01, 5.11802316e-01,\n",
      "        4.89853581e-01, 5.75454343e-01, 4.06861979e-01, 5.90750185e-02,\n",
      "        7.78925552e-04, 1.67240324e-01, 2.37303544e-01, 3.47018310e-01],\n",
      "       [2.50120991e-01, 4.83437487e-01, 8.70963119e-01, 2.49020948e-01,\n",
      "        7.95355354e-01, 6.35551954e-01, 3.16369376e-01, 4.92802889e-01,\n",
      "        7.47852113e-01, 1.93310297e-01, 3.11903078e-01, 5.96363085e-01,\n",
      "        8.91635640e-01, 6.64101653e-01, 1.03542752e-01, 2.84345734e-01,\n",
      "        2.38728060e-01, 1.32346110e-01, 8.07869545e-01, 5.36556960e-01],\n",
      "       [5.92715853e-01, 5.87965742e-01, 9.91321000e-01, 7.65934174e-01,\n",
      "        1.49924159e-01, 5.74175819e-01, 4.25584599e-01, 6.20600761e-01,\n",
      "        7.17836169e-01, 5.54298786e-01, 6.52859930e-01, 7.83738313e-01,\n",
      "        7.36074574e-01, 9.87011936e-01, 3.30631223e-01, 7.33893115e-01,\n",
      "        7.26533113e-01, 7.02570588e-01, 8.86475244e-01, 6.57481524e-01],\n",
      "       [9.71769360e-01, 4.73780271e-01, 9.83919908e-01, 4.20837489e-01,\n",
      "        4.15274556e-01, 7.51458475e-02, 7.54735074e-01, 9.69899304e-01,\n",
      "        7.05070332e-01, 9.53991797e-01, 2.05075522e-01, 4.32847610e-01,\n",
      "        3.42771149e-01, 2.30194268e-01, 1.27366315e-01, 5.89599096e-01,\n",
      "        4.19526294e-01, 6.74850576e-01, 3.32001912e-01, 3.55394185e-01],\n",
      "       [5.65433509e-01, 1.19032290e-01, 8.25677788e-01, 9.09540965e-01,\n",
      "        4.11374802e-01, 9.51898045e-01, 3.28041463e-01, 5.31797277e-01,\n",
      "        2.87846404e-02, 8.56136727e-01, 3.31313840e-01, 6.21986302e-01,\n",
      "        8.13519641e-01, 1.39830385e-01, 7.66179794e-01, 9.99536984e-01,\n",
      "        3.69836770e-01, 6.21652523e-01, 4.59009296e-01, 6.75045031e-01]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> X.shape\n",
      "(5, 40)\n",
      "ipdb> delta.shape\n",
      "(5, 20)\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6645736f37f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_deriv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m'grad_W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c5b785d7ae89>\u001b[0m in \u001b[0;36mbackwards\u001b[0;34m(delta, params, name, activation_deriv)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# then compute the derivative W,b, and X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# store the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c5b785d7ae89>\u001b[0m in \u001b[0;36mbackwards\u001b[0;34m(delta, params, name, activation_deriv)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# then compute the derivative W,b, and X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# store the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we use random values to test your implementation \n",
    "# independent of previous questions\n",
    "n, c1, c2 = 5, 40, 20 \n",
    "delta = np.random.rand(n, c2)\n",
    "name = 'layer1'\n",
    "params = {\n",
    "    'W'+name: np.random.rand(c1, c2),\n",
    "    'b'+name: np.random.rand(c2),\n",
    "    'cache_'+name: (np.random.rand(n, c1), \n",
    "                     np.random.rand(n, c2), \n",
    "                     np.random.rand(n, c2))\n",
    "}\n",
    "\n",
    "grad = backwards(delta, params, name, linear_deriv)\n",
    "\n",
    "assert 'grad_W' + name in params\n",
    "assert 'grad_b' + name in params\n",
    "\n",
    "assert params['grad_W'+name].shape == params['W'+name].shape\n",
    "assert params['grad_b'+name].shape == params['b'+name].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "280b594e90a1971a9bdd98576d12d589",
     "grade": false,
     "grade_id": "cell-4fbc1b9ed142d412",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.4 Convolutional Layer [Extra Credit]\n",
    "**Note: We would recommend finishing other questions before attempting questions under this section (q2.4.1 and q2.4.2)**\n",
    "\n",
    "For now we have worked with linear layer in fully-connected networks. In practice, convolutional layers are commonly used to extract image feature. You will implement the forward and backawad propagation for convolutional layer in this subsection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af2b7a93584740057f9b89167b369658",
     "grade": false,
     "grade_id": "cell-56852c776e6811a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.4.1 [Extra Credit](5 points Autograder)\n",
    "Similar to Q2.2.1, implement `conv_forward` for a single convolutional layer with zero paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be891bcc9486e7726997d016f0ddbe4d",
     "grade": false,
     "grade_id": "cell-5e3595068856df10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_forward(X: np.ndarray, params: dict, name: str='',\n",
    "            stride: int=1, pad: int=0):\n",
    "    \"\"\"\n",
    "    Do a forward pass for a convolutional layer\n",
    "\n",
    "    [input]\n",
    "    * X -- input data [N x C x H x W]\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    * stride, pad -- convolution parameters\n",
    "    \n",
    "    [output]\n",
    "    * res -- output after a convolutional layer\n",
    "    \"\"\"\n",
    "    res = None\n",
    "    # get the layer parameters\n",
    "    w = params['W' + name] # Conv Filter weights [F x C x HH x WW]\n",
    "    b = params['b' + name] # Biases [F]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # store the input and convolution parameters\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, stride, pad)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ef548dc2118bc97ccaa12e5f42e1233",
     "grade": true,
     "grade_id": "q2_4_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_shape = np.array((2, 3, 4, 4))\n",
    "w_shape = np.array((3, 3, 4, 4))\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape), dtype=np.float64).reshape(*x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape), dtype=np.float64).reshape(*w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3, dtype=np.float64)\n",
    "\n",
    "params = {'WConv_layer1': w, 'bConv_layer1': b}\n",
    "y = conv_forward(np.array(x), params, 'Conv_layer1', stride=2, pad=1)\n",
    "assert 'cache_Conv_layer1' in params\n",
    "\n",
    "\n",
    "y_ref = np.array([[[[-0.08759809, -0.10987781],\n",
    "                              [-0.18387192, -0.2109216 ]],\n",
    "                             [[ 0.21027089,  0.21661097],\n",
    "                              [ 0.22847626,  0.23004637]],\n",
    "                             [[ 0.50813986,  0.54309974],\n",
    "                              [ 0.64082444,  0.67101435]]],\n",
    "                            [[[-0.98053589, -1.03143541],\n",
    "                              [-1.19128892, -1.24695841]],\n",
    "                             [[ 0.69108355,  0.66880383],\n",
    "                              [ 0.59480972,  0.56776003]],\n",
    "                             [[ 2.36270298,  2.36904306],\n",
    "                              [ 2.38090835,  2.38247847]]]], \n",
    "            )\n",
    "assert y.shape == y_ref.shape\n",
    "\n",
    "max_diff = np.max(np.abs((y_ref - y)))\n",
    "base = (np.abs(y_ref) + np.abs(y)).clip(np.finfo(float).eps).max()\n",
    "print(max_diff/base) # the difference should be less than 1e-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b64b7cf8c25311e16a741c7e7ee51b1",
     "grade": false,
     "grade_id": "cell-984447a667304ac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.4.2 [Extra Credit](5 points Autograder)\n",
    "Implement `conv_backword` for a single convolutional layer with zero paddings.\n",
    "Compute back-propagation for a single convolutional layer, given the original weights, the cached input, and given gradient with respect to the loss. Similar to Q2.3.1, you should return the gradient with respect to $X$ so you can feed it into the next layer. As a size check, your gradients should be the same dimensions as the original objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "058aa9f5d5dd5f8d6d878e39b2ff6ad1",
     "grade": false,
     "grade_id": "cell-cc53a61e4f74a964",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_backward(delta: np.ndarray, params: dict, name: str=''):\n",
    "    \"\"\"\n",
    "    Do a backwards pass for a convolutional layer\n",
    "\n",
    "    [input]\n",
    "    * delta -- errors to backprop\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    \n",
    "    [output]\n",
    "    * grad_X -- gradient w.r.t X\n",
    "    \"\"\"\n",
    "    grad_X, grad_W, grad_b = None, None, None\n",
    "    # everything you may need for this layer\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    X, stride, pad = params['cache_' + name]\n",
    "\n",
    "    # compute the derivative W,b, and X\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26f99e6b981139d80bd36c83512edfb9",
     "grade": true,
     "grade_id": "q2_4_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(5, 4, 16, 16)\n",
    "w = np.random.rand(8, 4, 7, 7)\n",
    "b = np.random.rand(8,)\n",
    "dout = np.random.rand(5, 8, 16, 16)\n",
    "\n",
    "params = {'WConv_layer1': w, 'bConv_layer1': b}\n",
    "y = conv_forward(x, params, 'Conv_layer1', stride=2, pad=3)\n",
    "dx = conv_backward(dout, params, 'Conv_layer1')\n",
    "\n",
    "assert x.shape == dx.shape\n",
    "assert params['grad_WConv_layer1'].shape == params['WConv_layer1'].shape\n",
    "assert params['grad_bConv_layer1'].shape == params['bConv_layer1'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e04921e3817ee468161e11b616345c",
     "grade": false,
     "grade_id": "cell-1ca2e358a302f02f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.5 Training Loop\n",
    "You will tend to see gradient descent in three forms: \"normal\", \"stochastic\" and \"batch\". \"Normal\" gradient descent aggregates the updates for the entire dataset before changing the weights. Stochastic gradient descent applies updates after every single data example. Batch gradient descent is a compromise, where random subsets of the full dataset are evaluated before applying the gradient update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "601aaa76aeb23129ba6f9a2c21979077",
     "grade": false,
     "grade_id": "cell-683134f646db0287",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.5.1 (10 points Autograder)\n",
    "Write a training loop that generates random batches, iterates over them for many iterations, does forward and backward propagation, and applies a gradient update step. Specifically, implement `get_random_batches` and `train` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f1b9f89609b51466428af088518dc94",
     "grade": false,
     "grade_id": "cell-1b13b59b8a93b801",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_random_batches(x: np.ndarray, y: np.ndarray, batch_size: int) -> list:\n",
    "    \"\"\"\n",
    "    Split x and y into random batches\n",
    "    \n",
    "    [input]\n",
    "    * x -- training samples\n",
    "    * y -- training lables\n",
    "    * batch_size -- batch size\n",
    "    \n",
    "    [output]\n",
    "    * batches -- a list of [(batch1_x,batch1_y)...]\n",
    "    \"\"\"\n",
    "    # \n",
    "    # return \n",
    "    batches = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57753a919c44d52fca39d50d1596e331",
     "grade": true,
     "grade_id": "q2_5_1_random_batches",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n, c1, c2 = 20, 100, 5\n",
    "batch_size = 3\n",
    "x = np.random.rand(n, c1)\n",
    "y = np.random.rand(n, c2)\n",
    "batches = get_random_batches(x, y, batch_size)\n",
    "assert type(batches) == list\n",
    "assert len(batches) >= 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "536e781d5a4a0684f44501d32a2be83d",
     "grade": false,
     "grade_id": "cell-96bdeec247659603",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(x: np.ndarray, y: np.ndarray, params: dict, batch_size: int = 5,\n",
    "          max_iters: int = 500, learning_rate: float=1e-3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the network with two sequential layers: \n",
    "    (1) one layer named \"layer1\" with sigmoid activation\n",
    "    (2) one layer named \"output\" with softmax activation\n",
    "\n",
    "    [input]\n",
    "    * x -- training samples\n",
    "    * y -- training lables\n",
    "    * params -- a dictionary containing initial parameters\n",
    "    * batch_size -- batch size\n",
    "    * max_iters -- total number of iterations\n",
    "    * learning_rate -- learning rate\n",
    "    \n",
    "    [output]\n",
    "    * total_loss, avg_acc -- loss and accuracy for the last iteration\n",
    "    \"\"\"\n",
    "\n",
    "    batches = get_random_batches(x, y, batch_size)\n",
    "\n",
    "    for itr in range(max_iters):\n",
    "        total_loss = 0\n",
    "        avg_acc = 0\n",
    "        for xb, yb in batches:\n",
    "\n",
    "            # forward\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # loss\n",
    "            # be sure to add loss and accuracy to epoch totals\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # backward\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # apply gradient\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        if itr % 100 == 0:\n",
    "            print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(\n",
    "                itr, total_loss, avg_acc))\n",
    "    return total_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98a7a8c490dcb6f8e4d87b995c2b59a0",
     "grade": true,
     "grade_id": "q2_5_1_train",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Successulf implementation of dependent functions are required to get full score for the `train` function\n",
    "\n",
    "# create inputs\n",
    "g0 = np.random.multivariate_normal([3.6,40],[[0.05,0],[0,10]],10)\n",
    "g1 = np.random.multivariate_normal([3.9,10],[[0.01,0],[0,5]],10)\n",
    "g2 = np.random.multivariate_normal([3.4,30],[[0.25,0],[0,5]],10)\n",
    "g3 = np.random.multivariate_normal([2.0,10],[[0.5,0],[0,10]],10)\n",
    "x = np.vstack([g0,g1,g2,g3])\n",
    "\n",
    "# create labels\n",
    "y_idx = np.array([0 for _ in range(10)] + [1 for _ in range(10)] + [2 for _ in range(10)] + [3 for _ in range(10)])\n",
    "\n",
    "# turn labels to one_hot\n",
    "y = np.zeros((y_idx.shape[0],y_idx.max()+1))\n",
    "y[np.arange(y_idx.shape[0]),y_idx] = 1\n",
    "\n",
    "# parameters in a dictionary\n",
    "params = {}\n",
    "# initialize a layer\n",
    "initialize_weights(2,25,params,'layer1')\n",
    "initialize_weights(25,4,params,'output')\n",
    "\n",
    "# train the two-layer neural network\n",
    "total_loss, avg_acc = train(x, y, params, batch_size=5, max_iters=500, learning_rate=1e-3)\n",
    "print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(500, total_loss, avg_acc))\n",
    "\n",
    "# with default settings, you should get loss < 35 and accuracy > 75%\n",
    "assert total_loss < 35 and avg_acc > 0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b943a4dd0f05ea3ee38ff8cc6946b384",
     "grade": false,
     "grade_id": "cell-912ef865c23093b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.6 Numerical Gradient Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8752d6c61f4e0621b09ea7f86c779cd9",
     "grade": false,
     "grade_id": "cell-da874ad741d95189",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.6.1 (15 points Autograder)\n",
    "Implement the `centeral_differences_gradient` function. Instead of using the analytical gradients computed from the chain rule, add $\\epsilon$ offset to each element in the weights, and compute the numerical gradient of the loss with central differences. Central differences is just $\\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}$. Remember, this needs to be done for each scalar dimension in all of your weights independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5533bfa73539c91977ba16fb704b7bb7",
     "grade": false,
     "grade_id": "cell-2e7c04b3366231a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def centeral_differences_gradient(params: dict, eps = 1e-6):\n",
    "    \"\"\"\n",
    "    Compute the estimated gradients using central difference\n",
    "    \n",
    "    Hint:\n",
    "    please feel free to reuse the functions above\n",
    "    \"\"\"\n",
    "    for k, v in params.items():\n",
    "        if '_' in k:\n",
    "            continue\n",
    "        # we have a real parameter!\n",
    "        # for each value inside the parameter\n",
    "        #   subtract epsilon\n",
    "        #   run the forward function by \n",
    "        #   get the loss\n",
    "        #   add epsilon\n",
    "        #   run the forward function\n",
    "        #   get the loss\n",
    "        #   compute derivative with central diffs\n",
    "    \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31af9fe8cf792ec14a7564770deccb40",
     "grade": true,
     "grade_id": "q2_6_1",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the analytical gradients\n",
    "h1 = forward(x,params,'layer1')\n",
    "probs = forward(h1,params,'output',softmax)\n",
    "delta1 = probs\n",
    "delta1[np.arange(probs.shape[0]),y_idx] -= 1\n",
    "\n",
    "delta2 = backwards(delta1,params,'output',linear_deriv)\n",
    "backwards(delta2,params,'layer1',sigmoid_deriv)\n",
    "\n",
    "import copy\n",
    "params_orig = copy.deepcopy(params)\n",
    "\n",
    "# Compute the estimated gradient using central difference\n",
    "centeral_differences_gradient(params)\n",
    "\n",
    "total_error = 0\n",
    "for k in params.keys():\n",
    "    if 'grad_' in k:\n",
    "        # relative error\n",
    "        err = np.abs(params[k] - params_orig[k])/np.maximum(np.abs(params[k]),np.abs(params_orig[k]))\n",
    "        err = err.sum()\n",
    "        total_error += err\n",
    "# should be less than 1e-4\n",
    "assert 0. < total_error < 1e-4"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "54955deeadaafb0b8557ccc212e60e9b012c0994cf03c4af428539b6234184e1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
